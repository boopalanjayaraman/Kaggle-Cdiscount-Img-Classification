{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import bson\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.data import imread\n",
    "import multiprocessing as mp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NCORE = 2\n",
    "#all_categories = mp.Manager().list()\n",
    "\n",
    "all_categories = list()\n",
    "all_categories_array = np.array([])\n",
    "\n",
    "#categories to int dictionary\n",
    "categ_to_int = {}\n",
    "int_to_categ = {}\n",
    "\n",
    "#total number of items in the list\n",
    "n_train = 7069896 #from kaggle page\n",
    "n_test = 1768182 #from kaggle page\n",
    "n_example = 100 #from kaggle page\n",
    "\n",
    "all_categories_filename_format = 'allcategoriesdata_{0}.p'\n",
    "train_data_batch_file_format = 'training_batches/{0}/train_{0}_{1}_{2}.jpeg'\n",
    "test_data_batch_file_format = 'testing_batches/{0}/test_{0}_{1}_{2}.jpeg'\n",
    "\n",
    "train_category_folder_path_format = 'training_batches/{0}'\n",
    "test_category_folder_path_format = 'testing_batches/{0}'\n",
    "test_category_folder_name_format = 'folder_{0}'\n",
    "\n",
    "show_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def process_record_multicore_category(queue, iolock):\n",
    "    while True:\n",
    "        record = queue.get()\n",
    "        if record is None:\n",
    "            break\n",
    "        global all_categories\n",
    "        all_categories.append(record['category_id'])\n",
    "\n",
    "def process_all_categories(filepath):\n",
    "    \"\"\"\n",
    "    processes all categories and forms the list\n",
    "    : filepath: file path\n",
    "    \"\"\"\n",
    "    process_filename = filepath[filepath.rfind('/')+1:]\n",
    "    filename_suffix = process_filename.replace('.bson','')\n",
    "    categories_filename = all_categories_filename_format.format(filename_suffix)\n",
    "    if os.path.isfile(categories_filename):\n",
    "        print('File already exists. Seems already it is processed.')\n",
    "        return\n",
    "    \n",
    "    global all_categories\n",
    "    \n",
    "    #queue = mp.Queue(maxsize=NCORE)\n",
    "    #iolock = mp.Lock()\n",
    "    #pool = mp.Pool(NCORE, initializer=process_record_multicore_category, initargs=(queue, iolock))\n",
    "    \n",
    "    #loading data from file\n",
    "    data = bson.decode_file_iter(open(filepath, 'rb'))\n",
    "    \n",
    "    print('Starting to go through the file. Time: {0}'.format(time.ctime()))\n",
    "    for c, record in enumerate(data):\n",
    "        #queue.put(record)\n",
    "        all_categories.append(record['category_id'])\n",
    "        if c % 100000 ==0:\n",
    "            print ('records processed: {0}, time: {1}'.format(c, time.ctime()))\n",
    "    \n",
    "    # tell workers we're done and join the stuff\n",
    "    #for _ in range(NCORE):\n",
    "    #    queue.put(None)\n",
    "    #pool.close()\n",
    "    #pool.join()\n",
    "    print('File is processed. Time: {0}'.format(time.ctime()))\n",
    "    \n",
    "    all_categories_array = np.array(list(set(all_categories)))\n",
    "\n",
    "    #process the categories and save them\n",
    "    process_all_categories_array(all_categories_array, categories_filename)\n",
    "    print('all categories processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data record preprocess sub-function \n",
    "def process_record_train(record):\n",
    "    \"\"\"\n",
    "    processes each record from the training / test file during preprocessing function execution for training dataset\n",
    "    : record: record to be processed\n",
    "    : return: void\n",
    "    \"\"\" \n",
    "    product_id = record['_id']\n",
    "    category_id = record['category_id']\n",
    "    for e, pic in enumerate(record['imgs']):\n",
    "        picture = pic['picture']\n",
    "        filepath = train_data_batch_file_format.format(category_id,product_id, e)\n",
    "        if os.path.isfile(filepath):\n",
    "            continue\n",
    "        with open(filepath, 'wb') as w:\n",
    "            w.write(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data record preprocess sub-function for test data set\n",
    "def process_record_test(record, folder_id):\n",
    "    \"\"\"\n",
    "    processes each record from the training / test file during preprocessing function execution for test data set\n",
    "    : record: record to be processed\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    product_id = record['_id']  \n",
    "    for e, pic in enumerate(record['imgs']):\n",
    "        picture = pic['picture']\n",
    "        filepath = test_data_batch_file_format.format(folder_id,product_id, e)\n",
    "        if os.path.isfile(filepath):\n",
    "            continue\n",
    "        with open(filepath, 'wb') as w:\n",
    "            w.write(picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data preprocess function \n",
    "def process_training_file(data, enum_start=None, limit = None, file_suffix=''):\n",
    "    \"\"\"\n",
    "    processes the training file and saves them to batch files for loading them later\n",
    "    : filepath: path of the training file\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    #create all folders for categories\n",
    "    for categ, i in categ_to_int.items():\n",
    "        directory = train_category_folder_path_format.format(categ)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    \n",
    "    #loading data from file\n",
    "    #print('Starting to go through the Set. Time: {0}'.format(time.ctime()))\n",
    "    \n",
    "    init =  0 if enum_start == None else enum_start\n",
    "    for c, record in enumerate(data, start=init):\n",
    "        if(c % show_every ==0):\n",
    "            print('processed records: {0}, Time: {1}'.format(c, time.ctime()))\n",
    "        if(c > limit):\n",
    "            break\n",
    "        process_record_train(record)\n",
    "        \n",
    "    #print('File is processed. Time: {0}'.format(time.ctime()))\n",
    "    #print('Preprocessing is done and saved. Time: {0}'.format(time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test data preprocess function \n",
    "def process_test_file(data, enum_start=None, limit=None, file_suffix=''):\n",
    "    \"\"\"\n",
    "    processes test file and saves the output to batch files for loading them later\n",
    "    : filepath: path of the test file on disk\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    #create folder for each 10000 images\n",
    "    folder_name = test_category_folder_name_format.format(file_suffix)\n",
    "    directory = test_category_folder_path_format.format(folder_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    #loading data from file\n",
    "    #print('TestFile: Starting to go through the Set. Time: {0}'.format(time.ctime()))\n",
    "    \n",
    "    init =  0 if enum_start == None else enum_start\n",
    "    \n",
    "    for c, record in enumerate(data, start=init):\n",
    "        if(c % show_every==0):\n",
    "            print('processed records: {0}, Time: {1}'.format(c, time.ctime()))\n",
    "        if(c > limit):\n",
    "            break\n",
    "        process_record_test(record, folder_name)\n",
    "    \n",
    "    #print('TestFile: File is processed. Time: {0}'.format(time.ctime()))\n",
    "    #print('Preprocessing is done and saved. Time: {0}'.format(time.ctime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_all_categories_array(all_categories_array, processed_filename):\n",
    "    \"\"\"\n",
    "    processes all categories found in training data and creates dictionaries for faster reference\n",
    "    : all_categories_array: array that contains all categories to form one hot encoding\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    global categ_to_int, int_to_categ\n",
    "    categories_length = len(all_categories_array)\n",
    "    categ_to_int = { categ:idx for idx, categ in enumerate(all_categories_array) }\n",
    "    int_to_categ = { idx:categ for idx, categ in enumerate(all_categories_array) }\n",
    "    \n",
    "    pickle.dump((categ_to_int, int_to_categ), open(processed_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_categ_to_int_dicts(data_file_path):\n",
    "    \"\"\"\n",
    "    restores categ_to_int and int_to_categ object dictionaries from saved state files if exist\n",
    "    : data_file_path: actual data file path - to represent the mode (train or train example)\n",
    "    \"\"\"\n",
    "    process_filename = data_file_path[data_file_path.rfind('/')+1:]\n",
    "    filename_suffix = process_filename.replace('.bson','')\n",
    "    categories_filename = all_categories_filename_format.format(filename_suffix)\n",
    "    \n",
    "    with open(categories_filename, 'rb') as f:\n",
    "        \n",
    "        global categ_to_int, int_to_categ\n",
    "        \n",
    "        categ_to_int, int_to_categ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_label(original_label, label_length, one_hot_labels):\n",
    "    \"\"\"\n",
    "    creates one hot label for a given original label value. A sub function for multi core processing of one hot encode function\n",
    "    : label_length: length of label to initialize the array\n",
    "    : one_hot_labels: the array that contains all one hot label\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    one_hot_label = np.zeros(label_length, dtype='int16')\n",
    "    \n",
    "    #commenting below line since now conversion to index happens while preparing the matrix.\n",
    "    #one_hot_label[categ_to_int[original_label]] = 1\n",
    "    #so changing it to\n",
    "    one_hot_label[original_label] = 1\n",
    "    \n",
    "    one_hot_labels.append(one_hot_label)\n",
    "\n",
    "def one_hot_encode(data_batch):\n",
    "    \"\"\"\n",
    "    creates one hot encoded label for the given data batch using multi-core processing\n",
    "    : data_batch: the sub-section of original final training data\n",
    "    : return: array of one hot encoded label\n",
    "    \"\"\"\n",
    "    one_hot_labels = list()\n",
    "    label_length = len(categ_to_int)\n",
    "    \n",
    "    #print(data_batch)\n",
    "    \n",
    "    for i in range(len(data_batch)):\n",
    "        original_label = data_batch[i][1] # category column\n",
    "        create_one_hot_label(original_label, label_length, one_hot_labels)\n",
    "\n",
    "    one_hot_labels = np.array(list(one_hot_labels))\n",
    "    \n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Seems already it is processed.\n"
     ]
    }
   ],
   "source": [
    "#process_all_categories('data/train_example.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multicore processing Queue, Lock, and Pool have been initialized and set up.\n",
      "The data file has been loaded.\n",
      "Starting to go through the file. Time: Tue Oct 10 19:52:25 2017\n",
      "File is processed. Time: Tue Oct 10 19:52:25 2017\n",
      "Preprocessing is done and saved. Time: Tue Oct 10 19:52:25 2017\n"
     ]
    }
   ],
   "source": [
    "#test with training example file\n",
    "\n",
    "#process_training_file('data/train_example.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final_data_array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process_training_file('data/train.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load_categ_to_int_dicts('data/train_example.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to go through the file. Time: Fri Oct 20 14:01:56 2017\n",
      "records processed: 0, time: Fri Oct 20 14:01:56 2017\n",
      "records processed: 100000, time: Fri Oct 20 14:02:06 2017\n",
      "records processed: 200000, time: Fri Oct 20 14:02:18 2017\n",
      "records processed: 300000, time: Fri Oct 20 14:02:38 2017\n",
      "records processed: 400000, time: Fri Oct 20 14:02:50 2017\n",
      "records processed: 500000, time: Fri Oct 20 14:03:01 2017\n",
      "records processed: 600000, time: Fri Oct 20 14:03:21 2017\n",
      "records processed: 700000, time: Fri Oct 20 14:04:09 2017\n",
      "records processed: 800000, time: Fri Oct 20 14:04:19 2017\n",
      "records processed: 900000, time: Fri Oct 20 14:04:30 2017\n",
      "records processed: 1000000, time: Fri Oct 20 14:04:40 2017\n",
      "records processed: 1100000, time: Fri Oct 20 14:04:50 2017\n",
      "records processed: 1200000, time: Fri Oct 20 14:05:00 2017\n",
      "records processed: 1300000, time: Fri Oct 20 14:05:13 2017\n",
      "records processed: 1400000, time: Fri Oct 20 14:05:23 2017\n",
      "records processed: 1500000, time: Fri Oct 20 14:05:33 2017\n",
      "records processed: 1600000, time: Fri Oct 20 14:05:43 2017\n",
      "records processed: 1700000, time: Fri Oct 20 14:05:53 2017\n",
      "records processed: 1800000, time: Fri Oct 20 14:06:02 2017\n",
      "records processed: 1900000, time: Fri Oct 20 14:06:12 2017\n",
      "records processed: 2000000, time: Fri Oct 20 14:06:22 2017\n",
      "records processed: 2100000, time: Fri Oct 20 14:06:33 2017\n",
      "records processed: 2200000, time: Fri Oct 20 14:06:43 2017\n",
      "records processed: 2300000, time: Fri Oct 20 14:06:54 2017\n",
      "records processed: 2400000, time: Fri Oct 20 14:07:05 2017\n",
      "records processed: 2500000, time: Fri Oct 20 14:07:20 2017\n",
      "records processed: 2600000, time: Fri Oct 20 14:07:31 2017\n",
      "records processed: 2700000, time: Fri Oct 20 14:07:41 2017\n",
      "records processed: 2800000, time: Fri Oct 20 14:07:51 2017\n",
      "records processed: 2900000, time: Fri Oct 20 14:08:02 2017\n",
      "records processed: 3000000, time: Fri Oct 20 14:08:18 2017\n",
      "records processed: 3100000, time: Fri Oct 20 14:08:28 2017\n",
      "records processed: 3200000, time: Fri Oct 20 14:08:38 2017\n",
      "records processed: 3300000, time: Fri Oct 20 14:08:48 2017\n",
      "records processed: 3400000, time: Fri Oct 20 14:08:58 2017\n",
      "records processed: 3500000, time: Fri Oct 20 14:09:08 2017\n",
      "records processed: 3600000, time: Fri Oct 20 14:09:19 2017\n",
      "records processed: 3700000, time: Fri Oct 20 14:09:28 2017\n",
      "records processed: 3800000, time: Fri Oct 20 14:09:38 2017\n",
      "records processed: 3900000, time: Fri Oct 20 14:09:48 2017\n",
      "records processed: 4000000, time: Fri Oct 20 14:09:57 2017\n",
      "records processed: 4100000, time: Fri Oct 20 14:10:07 2017\n",
      "records processed: 4200000, time: Fri Oct 20 14:10:18 2017\n",
      "records processed: 4300000, time: Fri Oct 20 14:10:28 2017\n",
      "records processed: 4400000, time: Fri Oct 20 14:10:38 2017\n",
      "records processed: 4500000, time: Fri Oct 20 14:10:49 2017\n",
      "records processed: 4600000, time: Fri Oct 20 14:10:59 2017\n",
      "records processed: 4700000, time: Fri Oct 20 14:11:09 2017\n",
      "records processed: 4800000, time: Fri Oct 20 14:11:20 2017\n",
      "records processed: 4900000, time: Fri Oct 20 14:11:30 2017\n",
      "records processed: 5000000, time: Fri Oct 20 14:11:40 2017\n",
      "records processed: 5100000, time: Fri Oct 20 14:11:51 2017\n",
      "records processed: 5200000, time: Fri Oct 20 14:12:00 2017\n",
      "records processed: 5300000, time: Fri Oct 20 14:12:10 2017\n",
      "records processed: 5400000, time: Fri Oct 20 14:12:20 2017\n",
      "records processed: 5500000, time: Fri Oct 20 14:12:30 2017\n",
      "records processed: 5600000, time: Fri Oct 20 14:12:41 2017\n",
      "records processed: 5700000, time: Fri Oct 20 14:12:51 2017\n",
      "records processed: 5800000, time: Fri Oct 20 14:13:02 2017\n",
      "records processed: 5900000, time: Fri Oct 20 14:13:19 2017\n",
      "records processed: 6000000, time: Fri Oct 20 14:13:30 2017\n",
      "records processed: 6100000, time: Fri Oct 20 14:13:41 2017\n",
      "records processed: 6200000, time: Fri Oct 20 14:13:50 2017\n",
      "records processed: 6300000, time: Fri Oct 20 14:14:01 2017\n",
      "records processed: 6400000, time: Fri Oct 20 14:14:10 2017\n",
      "records processed: 6500000, time: Fri Oct 20 14:14:21 2017\n",
      "records processed: 6600000, time: Fri Oct 20 14:14:31 2017\n",
      "records processed: 6700000, time: Fri Oct 20 14:14:41 2017\n",
      "records processed: 6800000, time: Fri Oct 20 14:14:52 2017\n",
      "records processed: 6900000, time: Fri Oct 20 14:15:02 2017\n",
      "records processed: 7000000, time: Fri Oct 20 14:15:13 2017\n",
      "File is processed. Time: Fri Oct 20 14:15:21 2017\n",
      "all categories processed.\n"
     ]
    }
   ],
   "source": [
    "process_all_categories('data/train.bson')\n",
    "#process_all_categories('train_example.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dictionaries - categ_to_int and int_to_categ from files to objects\n",
    "load_categ_to_int_dicts('data/train.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categ_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test_batches(filepath, override_batch=None):\n",
    "    \"\"\"\n",
    "    preprocesses batches and saves them in batches to end up losing data due to long running processes\n",
    "    : filepath: path of file to be processed\n",
    "    \"\"\"\n",
    "    input_data = bson.decode_file_iter(open(filepath, 'rb'))\n",
    "    \n",
    "    limit = 10000\n",
    "    batches_count = int(n_test / limit)\n",
    "    batch_range = batches_count if override_batch is None else override_batch\n",
    "    for batch_idx in range(batch_range):\n",
    "        print('starting with batch: {0}'.format(batch_idx))\n",
    "        process_test_file(input_data, enum_start=batch_idx*limit, limit=(batch_idx+1)*limit, file_suffix=batch_idx)\n",
    "        \n",
    "    print('all test files are preprocessed. cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_training_batches(filepath, override_batch=None):\n",
    "    \"\"\"\n",
    "    preprocesses batches and saves them in batches to end up losing data due to long running processes\n",
    "    : filepath: path of file to be processed\n",
    "    \"\"\"\n",
    "    input_data = bson.decode_file_iter(open(filepath, 'rb'))\n",
    "    \n",
    "    limit = 10000\n",
    "    batches_count = int(n_train / limit)\n",
    "    batch_range = batches_count if override_batch is None else override_batch\n",
    "    for batch_idx in range(batch_range):\n",
    "        print('starting with batch: {0}'.format(batch_idx))\n",
    "        process_training_file(input_data, enum_start=batch_idx*limit, limit=(batch_idx+1)*limit, file_suffix=batch_idx)\n",
    "\n",
    "    print('all training files are preprocessed. cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with batch: 0\n",
      "processed records: 0, Time: Fri Oct 20 15:46:32 2017\n",
      "processed records: 10000, Time: Fri Oct 20 15:46:51 2017\n",
      "starting with batch: 1\n",
      "processed records: 10000, Time: Fri Oct 20 15:46:52 2017\n",
      "processed records: 20000, Time: Fri Oct 20 15:47:05 2017\n",
      "starting with batch: 2\n",
      "processed records: 20000, Time: Fri Oct 20 15:47:05 2017\n",
      "processed records: 30000, Time: Fri Oct 20 15:47:13 2017\n",
      "starting with batch: 3\n",
      "processed records: 30000, Time: Fri Oct 20 15:47:14 2017\n",
      "processed records: 40000, Time: Fri Oct 20 15:47:22 2017\n",
      "starting with batch: 4\n",
      "processed records: 40000, Time: Fri Oct 20 15:47:22 2017\n",
      "processed records: 50000, Time: Fri Oct 20 15:47:31 2017\n",
      "starting with batch: 5\n",
      "processed records: 50000, Time: Fri Oct 20 15:47:31 2017\n",
      "processed records: 60000, Time: Fri Oct 20 15:47:39 2017\n",
      "starting with batch: 6\n",
      "processed records: 60000, Time: Fri Oct 20 15:47:40 2017\n",
      "processed records: 70000, Time: Fri Oct 20 15:47:49 2017\n",
      "starting with batch: 7\n",
      "processed records: 70000, Time: Fri Oct 20 15:47:49 2017\n",
      "processed records: 80000, Time: Fri Oct 20 15:47:57 2017\n",
      "starting with batch: 8\n",
      "processed records: 80000, Time: Fri Oct 20 15:47:58 2017\n",
      "processed records: 90000, Time: Fri Oct 20 15:48:05 2017\n",
      "starting with batch: 9\n",
      "processed records: 90000, Time: Fri Oct 20 15:48:05 2017\n",
      "processed records: 100000, Time: Fri Oct 20 15:48:18 2017\n",
      "starting with batch: 10\n",
      "processed records: 100000, Time: Fri Oct 20 15:48:18 2017\n",
      "processed records: 110000, Time: Fri Oct 20 15:48:26 2017\n",
      "starting with batch: 11\n",
      "processed records: 110000, Time: Fri Oct 20 15:48:26 2017\n",
      "processed records: 120000, Time: Fri Oct 20 15:48:34 2017\n",
      "starting with batch: 12\n",
      "processed records: 120000, Time: Fri Oct 20 15:48:34 2017\n",
      "processed records: 130000, Time: Fri Oct 20 15:48:42 2017\n",
      "starting with batch: 13\n",
      "processed records: 130000, Time: Fri Oct 20 15:48:42 2017\n",
      "processed records: 140000, Time: Fri Oct 20 15:48:50 2017\n",
      "starting with batch: 14\n",
      "processed records: 140000, Time: Fri Oct 20 15:48:50 2017\n",
      "processed records: 150000, Time: Fri Oct 20 15:48:59 2017\n",
      "starting with batch: 15\n",
      "processed records: 150000, Time: Fri Oct 20 15:48:59 2017\n",
      "processed records: 160000, Time: Fri Oct 20 15:49:08 2017\n",
      "starting with batch: 16\n",
      "processed records: 160000, Time: Fri Oct 20 15:49:08 2017\n",
      "processed records: 170000, Time: Fri Oct 20 15:49:18 2017\n",
      "starting with batch: 17\n",
      "processed records: 170000, Time: Fri Oct 20 15:49:19 2017\n",
      "processed records: 180000, Time: Fri Oct 20 15:49:33 2017\n",
      "starting with batch: 18\n",
      "processed records: 180000, Time: Fri Oct 20 15:49:33 2017\n",
      "processed records: 190000, Time: Fri Oct 20 15:49:48 2017\n",
      "starting with batch: 19\n",
      "processed records: 190000, Time: Fri Oct 20 15:49:49 2017\n",
      "processed records: 200000, Time: Fri Oct 20 15:50:03 2017\n",
      "starting with batch: 20\n",
      "processed records: 200000, Time: Fri Oct 20 15:50:03 2017\n",
      "processed records: 210000, Time: Fri Oct 20 15:50:17 2017\n",
      "starting with batch: 21\n",
      "processed records: 210000, Time: Fri Oct 20 15:50:17 2017\n",
      "processed records: 220000, Time: Fri Oct 20 15:50:34 2017\n",
      "starting with batch: 22\n",
      "processed records: 220000, Time: Fri Oct 20 15:50:34 2017\n",
      "processed records: 230000, Time: Fri Oct 20 15:50:49 2017\n",
      "starting with batch: 23\n",
      "processed records: 230000, Time: Fri Oct 20 15:50:50 2017\n",
      "processed records: 240000, Time: Fri Oct 20 15:51:04 2017\n",
      "starting with batch: 24\n",
      "processed records: 240000, Time: Fri Oct 20 15:51:04 2017\n",
      "processed records: 250000, Time: Fri Oct 20 15:51:19 2017\n",
      "starting with batch: 25\n",
      "processed records: 250000, Time: Fri Oct 20 15:51:19 2017\n",
      "processed records: 260000, Time: Fri Oct 20 15:51:35 2017\n",
      "starting with batch: 26\n",
      "processed records: 260000, Time: Fri Oct 20 15:51:35 2017\n",
      "processed records: 270000, Time: Fri Oct 20 15:51:51 2017\n",
      "starting with batch: 27\n",
      "processed records: 270000, Time: Fri Oct 20 15:51:51 2017\n",
      "processed records: 280000, Time: Fri Oct 20 15:52:07 2017\n",
      "starting with batch: 28\n",
      "processed records: 280000, Time: Fri Oct 20 15:52:07 2017\n",
      "processed records: 290000, Time: Fri Oct 20 15:52:27 2017\n",
      "starting with batch: 29\n",
      "processed records: 290000, Time: Fri Oct 20 15:52:27 2017\n",
      "processed records: 300000, Time: Fri Oct 20 15:52:43 2017\n",
      "starting with batch: 30\n",
      "processed records: 300000, Time: Fri Oct 20 15:52:43 2017\n",
      "processed records: 310000, Time: Fri Oct 20 15:52:58 2017\n",
      "starting with batch: 31\n",
      "processed records: 310000, Time: Fri Oct 20 15:52:59 2017\n",
      "processed records: 320000, Time: Fri Oct 20 15:53:18 2017\n",
      "starting with batch: 32\n",
      "processed records: 320000, Time: Fri Oct 20 15:53:18 2017\n",
      "processed records: 330000, Time: Fri Oct 20 15:53:34 2017\n",
      "starting with batch: 33\n",
      "processed records: 330000, Time: Fri Oct 20 15:53:34 2017\n",
      "processed records: 340000, Time: Fri Oct 20 15:53:53 2017\n",
      "starting with batch: 34\n",
      "processed records: 340000, Time: Fri Oct 20 15:53:53 2017\n",
      "processed records: 350000, Time: Fri Oct 20 15:54:14 2017\n",
      "starting with batch: 35\n",
      "processed records: 350000, Time: Fri Oct 20 15:54:14 2017\n",
      "processed records: 360000, Time: Fri Oct 20 15:54:34 2017\n",
      "starting with batch: 36\n",
      "processed records: 360000, Time: Fri Oct 20 15:54:34 2017\n",
      "processed records: 370000, Time: Fri Oct 20 15:55:39 2017\n",
      "starting with batch: 37\n",
      "processed records: 370000, Time: Fri Oct 20 15:55:40 2017\n",
      "processed records: 380000, Time: Fri Oct 20 15:57:21 2017\n",
      "starting with batch: 38\n",
      "processed records: 380000, Time: Fri Oct 20 15:57:21 2017\n",
      "processed records: 390000, Time: Fri Oct 20 15:59:17 2017\n",
      "starting with batch: 39\n",
      "processed records: 390000, Time: Fri Oct 20 15:59:20 2017\n",
      "processed records: 400000, Time: Fri Oct 20 16:01:01 2017\n",
      "starting with batch: 40\n",
      "processed records: 400000, Time: Fri Oct 20 16:01:08 2017\n",
      "processed records: 410000, Time: Fri Oct 20 16:02:57 2017\n",
      "starting with batch: 41\n",
      "processed records: 410000, Time: Fri Oct 20 16:02:58 2017\n",
      "processed records: 420000, Time: Fri Oct 20 16:05:19 2017\n",
      "starting with batch: 42\n",
      "processed records: 420000, Time: Fri Oct 20 16:05:20 2017\n",
      "processed records: 430000, Time: Fri Oct 20 16:07:18 2017\n",
      "starting with batch: 43\n",
      "processed records: 430000, Time: Fri Oct 20 16:07:19 2017\n",
      "processed records: 440000, Time: Fri Oct 20 16:09:09 2017\n",
      "starting with batch: 44\n",
      "processed records: 440000, Time: Fri Oct 20 16:09:16 2017\n",
      "processed records: 450000, Time: Fri Oct 20 16:11:10 2017\n",
      "starting with batch: 45\n",
      "processed records: 450000, Time: Fri Oct 20 16:11:10 2017\n",
      "processed records: 460000, Time: Fri Oct 20 16:12:56 2017\n",
      "starting with batch: 46\n",
      "processed records: 460000, Time: Fri Oct 20 16:12:56 2017\n",
      "processed records: 470000, Time: Fri Oct 20 16:15:06 2017\n",
      "starting with batch: 47\n",
      "processed records: 470000, Time: Fri Oct 20 16:15:07 2017\n",
      "processed records: 480000, Time: Fri Oct 20 16:17:08 2017\n",
      "starting with batch: 48\n",
      "processed records: 480000, Time: Fri Oct 20 16:17:11 2017\n",
      "processed records: 490000, Time: Fri Oct 20 16:19:39 2017\n",
      "starting with batch: 49\n",
      "processed records: 490000, Time: Fri Oct 20 16:19:39 2017\n",
      "processed records: 500000, Time: Fri Oct 20 16:21:44 2017\n",
      "starting with batch: 50\n",
      "processed records: 500000, Time: Fri Oct 20 16:21:45 2017\n",
      "processed records: 510000, Time: Fri Oct 20 16:23:59 2017\n",
      "starting with batch: 51\n",
      "processed records: 510000, Time: Fri Oct 20 16:24:00 2017\n",
      "processed records: 520000, Time: Fri Oct 20 16:26:16 2017\n",
      "starting with batch: 52\n",
      "processed records: 520000, Time: Fri Oct 20 16:26:16 2017\n",
      "processed records: 530000, Time: Fri Oct 20 16:28:12 2017\n",
      "starting with batch: 53\n",
      "processed records: 530000, Time: Fri Oct 20 16:28:34 2017\n",
      "processed records: 540000, Time: Fri Oct 20 16:30:58 2017\n",
      "starting with batch: 54\n",
      "processed records: 540000, Time: Fri Oct 20 16:30:59 2017\n",
      "processed records: 550000, Time: Fri Oct 20 16:32:54 2017\n",
      "starting with batch: 55\n",
      "processed records: 550000, Time: Fri Oct 20 16:32:56 2017\n",
      "processed records: 560000, Time: Fri Oct 20 16:35:09 2017\n",
      "starting with batch: 56\n",
      "processed records: 560000, Time: Fri Oct 20 16:35:09 2017\n",
      "processed records: 570000, Time: Fri Oct 20 16:37:35 2017\n",
      "starting with batch: 57\n",
      "processed records: 570000, Time: Fri Oct 20 16:37:35 2017\n",
      "processed records: 580000, Time: Fri Oct 20 16:40:35 2017\n",
      "starting with batch: 58\n",
      "processed records: 580000, Time: Fri Oct 20 16:40:35 2017\n",
      "processed records: 590000, Time: Fri Oct 20 16:42:52 2017\n",
      "starting with batch: 59\n",
      "processed records: 590000, Time: Fri Oct 20 16:42:52 2017\n",
      "processed records: 600000, Time: Fri Oct 20 16:45:07 2017\n",
      "starting with batch: 60\n",
      "processed records: 600000, Time: Fri Oct 20 16:45:07 2017\n",
      "processed records: 610000, Time: Fri Oct 20 16:47:11 2017\n",
      "starting with batch: 61\n",
      "processed records: 610000, Time: Fri Oct 20 16:47:12 2017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-22fbd1c596d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess_training_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/train.bson'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-20e3cbbcc412>\u001b[0m in \u001b[0;36mpreprocess_training_batches\u001b[0;34m(filepath, override_batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'starting with batch: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mprocess_training_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_suffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all training files are preprocessed. cool!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-0c1ff9702f39>\u001b[0m in \u001b[0;36mprocess_training_file\u001b[0;34m(data, enum_start, limit, file_suffix)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprocess_record_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[1;31m#print('File is processed. Time: {0}'.format(time.ctime()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a526564865bc>\u001b[0m in \u001b[0;36mprocess_record_train\u001b[0;34m(record)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_training_batches('data/train.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with batch: 0\n",
      "processed records: 0, Time: Mon Oct 23 01:03:48 2017\n",
      "processed records: 10000, Time: Mon Oct 23 01:04:03 2017\n",
      "starting with batch: 1\n",
      "processed records: 10000, Time: Mon Oct 23 01:04:03 2017\n",
      "processed records: 20000, Time: Mon Oct 23 01:04:24 2017\n",
      "starting with batch: 2\n",
      "processed records: 20000, Time: Mon Oct 23 01:04:24 2017\n",
      "processed records: 30000, Time: Mon Oct 23 01:04:42 2017\n",
      "starting with batch: 3\n",
      "processed records: 30000, Time: Mon Oct 23 01:04:42 2017\n",
      "processed records: 40000, Time: Mon Oct 23 01:05:03 2017\n",
      "starting with batch: 4\n",
      "processed records: 40000, Time: Mon Oct 23 01:05:03 2017\n",
      "processed records: 50000, Time: Mon Oct 23 01:05:20 2017\n",
      "starting with batch: 5\n",
      "processed records: 50000, Time: Mon Oct 23 01:05:20 2017\n",
      "processed records: 60000, Time: Mon Oct 23 01:05:36 2017\n",
      "starting with batch: 6\n",
      "processed records: 60000, Time: Mon Oct 23 01:05:36 2017\n",
      "processed records: 70000, Time: Mon Oct 23 01:05:53 2017\n",
      "starting with batch: 7\n",
      "processed records: 70000, Time: Mon Oct 23 01:05:53 2017\n",
      "processed records: 80000, Time: Mon Oct 23 01:06:10 2017\n",
      "starting with batch: 8\n",
      "processed records: 80000, Time: Mon Oct 23 01:06:10 2017\n",
      "processed records: 90000, Time: Mon Oct 23 01:06:27 2017\n",
      "starting with batch: 9\n",
      "processed records: 90000, Time: Mon Oct 23 01:06:27 2017\n",
      "processed records: 100000, Time: Mon Oct 23 01:06:43 2017\n",
      "starting with batch: 10\n",
      "processed records: 100000, Time: Mon Oct 23 01:06:43 2017\n",
      "processed records: 110000, Time: Mon Oct 23 01:07:00 2017\n",
      "starting with batch: 11\n",
      "processed records: 110000, Time: Mon Oct 23 01:07:00 2017\n",
      "processed records: 120000, Time: Mon Oct 23 01:07:17 2017\n",
      "starting with batch: 12\n",
      "processed records: 120000, Time: Mon Oct 23 01:07:17 2017\n",
      "processed records: 130000, Time: Mon Oct 23 01:07:33 2017\n",
      "starting with batch: 13\n",
      "processed records: 130000, Time: Mon Oct 23 01:07:33 2017\n",
      "processed records: 140000, Time: Mon Oct 23 01:07:53 2017\n",
      "starting with batch: 14\n",
      "processed records: 140000, Time: Mon Oct 23 01:07:53 2017\n",
      "processed records: 150000, Time: Mon Oct 23 01:08:14 2017\n",
      "starting with batch: 15\n",
      "processed records: 150000, Time: Mon Oct 23 01:08:14 2017\n",
      "processed records: 160000, Time: Mon Oct 23 01:08:35 2017\n",
      "starting with batch: 16\n",
      "processed records: 160000, Time: Mon Oct 23 01:08:35 2017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dbc2554649b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess_test_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test.bson'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-146a1f29259f>\u001b[0m in \u001b[0;36mpreprocess_test_batches\u001b[0;34m(filepath, override_batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'starting with batch: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mprocess_test_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_suffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all test files are preprocessed. cool!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0accf7b18ae1>\u001b[0m in \u001b[0;36mprocess_test_file\u001b[0;34m(data, enum_start, limit, file_suffix)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprocess_record_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[1;31m#print('TestFile: File is processed. Time: {0}'.format(time.ctime()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-585fb3d9d45e>\u001b[0m in \u001b[0;36mprocess_record_test\u001b[0;34m(record, folder_id)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_test_batches('data/test.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
