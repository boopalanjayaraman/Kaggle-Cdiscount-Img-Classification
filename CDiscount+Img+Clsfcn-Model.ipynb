{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import bson\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.data import imread\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NCORE = 2\n",
    "\n",
    "all_categories_array = np.array([])\n",
    "\n",
    "#categories to int dictionary\n",
    "categ_to_int = {}\n",
    "int_to_categ = {}\n",
    "\n",
    "#total number of items in the list\n",
    "n_train = 7069896 #from kaggle page\n",
    "n_test = 1768182 #from kaggle page\n",
    "n_example = 100 #from kaggle page\n",
    "\n",
    "all_categories_filename_format = 'allcategoriesdata_{0}.p'\n",
    "train_data_batch_file_format = 'training_batches/{0}/train_{0}_{1}_{2}.jpeg'\n",
    "test_data_batch_file_format = 'testing_batches/{0}/test_{0}_{1}_{2}.jpeg'\n",
    "\n",
    "train_category_folder_path_format = 'training_batches/{0}'\n",
    "test_category_folder_path_format = 'testing_batches/{0}'\n",
    "test_category_folder_name_format = 'folder_{0}'\n",
    "\n",
    "show_every = 10000\n",
    "\n",
    "mini_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_categ_to_int_dicts(data_file_path):\n",
    "    \"\"\"\n",
    "    restores categ_to_int and int_to_categ object dictionaries from saved state files if exist\n",
    "    : data_file_path: actual data file path - to represent the mode (train or train example)\n",
    "    \"\"\"\n",
    "    process_filename = data_file_path[data_file_path.rfind('/')+1:]\n",
    "    filename_suffix = process_filename.replace('.bson','')\n",
    "    categories_filename = all_categories_filename_format.format(filename_suffix)\n",
    "    \n",
    "    with open(categories_filename, 'rb') as f:\n",
    "        \n",
    "        global categ_to_int, int_to_categ\n",
    "        \n",
    "        categ_to_int, int_to_categ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_label(original_label, label_length, one_hot_labels):\n",
    "    \"\"\"\n",
    "    creates one hot label for a given original label value. A sub function for multi core processing of one hot encode function\n",
    "    : label_length: length of label to initialize the array\n",
    "    : one_hot_labels: the array that contains all one hot label\n",
    "    : return: void\n",
    "    \"\"\"\n",
    "    one_hot_label = np.zeros(label_length, dtype='int16')\n",
    "    one_hot_label[categ_to_int[original_label]] = 1\n",
    "    one_hot_labels.append(one_hot_label)\n",
    "\n",
    "def one_hot_encode(data_batch):\n",
    "    \"\"\"\n",
    "    creates one hot encoded label for the given data batch using multi-core processing\n",
    "    : data_batch: the sub-section of original final training data\n",
    "    : return: array of one hot encoded label\n",
    "    \"\"\"\n",
    "    one_hot_labels = list()\n",
    "    label_length = len(categ_to_int)\n",
    "    #print(data_batch)\n",
    "    for i in range(len(data_batch)):\n",
    "        original_label = data_batch[i][0] # 0 - category column\n",
    "        create_one_hot_label(original_label, label_length, one_hot_labels)\n",
    "\n",
    "    one_hot_labels = np.array(list(one_hot_labels))\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dictionaries - categ_to_int and int_to_categ from files to objects\n",
    "load_categ_to_int_dicts('data/train.bson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categ_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (180, 180, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    xmax = 255 #image max value\n",
    "    return x.astype(np.float)/float(xmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'training_batches/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Load function done\n"
     ]
    }
   ],
   "source": [
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    loads the image from the given path, crops if it is not 180x180 and returns the image data\n",
    "    : path: image file path\n",
    "    : returns: resized image data\n",
    "    \"\"\"\n",
    "    img = imread(path)\n",
    "    img = img / 255.0\n",
    "    if (img.shape[0] == 180) & (img.shape[1] == 180):\n",
    "        return img\n",
    "    else:\n",
    "        short_edge = min(img.shape[:2])\n",
    "        yy = int((img.shape[0] - short_edge) / 2)\n",
    "        xx = int((img.shape[1] - short_edge) / 2)\n",
    "        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "        # resize to 180, 180\n",
    "        resized_img = skimage.transform.resize(crop_img, (180, 180), mode='constant')\n",
    "        return resized_img\n",
    "\n",
    "'''Test Method below'''\n",
    "#load_image('training_batches/1000000237/train_1000000237_12600_0.jpeg')\n",
    "print('Image Load function done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_label_mapping = 'file_label_mapping.p'\n",
    "file_path_format = '{0}/{1}/{2}'\n",
    "label_folder_format = '{0}/{1}'\n",
    "\n",
    "def fetch_filenames_labels_train(folder_path):\n",
    "    \"\"\"\n",
    "    fetches all the filenames and their labels and dumps them into a pickle files\n",
    "    : folder_path: path of the parent folder\n",
    "    : returns: void\n",
    "    \"\"\"\n",
    "    contents = os.listdir(folder_path)\n",
    "    all_labels = [each for each in contents if os.path.isdir(label_folder_format.format(folder_path, each))]\n",
    "    labels = list()\n",
    "    inputs = list()\n",
    "    for label_folder in all_labels:\n",
    "        img_files = os.listdir(label_folder_format.format(folder_path, label_folder))\n",
    "        inputs.extend([label_folder_format.format(folder_path, label_folder, each) for each in img_files])\n",
    "        labels.extend([label_folder for each in img_files])\n",
    "    pickle.dump((inputs, labels), open(file_label_mapping, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch_filenames_labels_train('training_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_filenames_labels_train(file_path):\n",
    "    \"\"\"\n",
    "    loads the pickle file that has information of image file paths and their respective labels \n",
    "    : file_path: pickle file path\n",
    "    : returns: inputs (file paths) and labels\n",
    "    \"\"\"\n",
    "    if(os.path.exists(file_path)):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            inputs, labels = pickle.load(f)\n",
    "            return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, labels = restore_filenames_labels_train(file_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_array = np.array(inputs)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def get_training_val_test_sets(inputs_array, labels_array):\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.4)\n",
    "    train_idx, val_idx = next(splitter.split(inputs_array, labels_array))\n",
    "\n",
    "    half_val_len = int(len(val_idx)/2)\n",
    "    val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "    train_x, train_y = inputs_array[train_idx], labels_array[train_idx]\n",
    "    val_x, val_y = inputs_array[val_idx], labels_array[val_idx]\n",
    "    test_x, test_y = inputs_array[test_idx], labels_array[test_idx]\n",
    "    \n",
    "    result = [[train_x, train_y], [val_x, val_y], [test_x, test_y]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inspired by resnet50 (infact, trying to recreate resnet50)\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"\n",
    "    creates an identity block. Identity layer is a layer that has no conv layer at shortcut\n",
    "    : input_tensor: input tensor\n",
    "    : kernel_size: default 3, kernel size of the middle layer\n",
    "    : filters: list of integers, filter sizes of three conv layers\n",
    "    : stage: current stage, integer, used for creating names\n",
    "    : block: current block, character, used for creating names\n",
    "    \"\"\"\n",
    "    filter1, filter2, filter3 = filters\n",
    "    bn_axis = 3\n",
    "    \n",
    "    conv_name = 'res_{0}_{1}_branch_'.format(str(stage), block)\n",
    "    bn_name = 'bn_{0}_{1}_branch_'.format(str(stage), block)\n",
    "    \n",
    "    #kernel_size is fed, strides=(1,1) default, padding=valid default\n",
    "    x = tf.layers.conv2d(input_tensor, filter1, (1,1), name=conv_name + '2a') \n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2a')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filter2, kernel_size=kernel_size, padding='same', name= conv_name + '2b')\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2b')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filter3, (1,1), name=conv_name + '2c')\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2c')\n",
    "    \n",
    "    x = tf.add(x, input_tensor) #short cut connection\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"\n",
    "    creates an conv block. Conv block layer is a layer that has a conv layer at shortcut\n",
    "    : input_tensor: input tensor\n",
    "    : kernel_size: default 3, kernel size of the middle layer\n",
    "    : filters: list of integers, filter sizes of three conv layers\n",
    "    : stage: current stage, integer, used for creating names\n",
    "    : block: current block, character, used for creating names\n",
    "    : strides: strides that kernels take\n",
    "    \"\"\"\n",
    "    filter1, filter2, filter3 = filters\n",
    "    bn_axis = 3\n",
    "    \n",
    "    conv_name = 'res_{0}_{1}_branch_'.format(str(stage), block)\n",
    "    bn_name = 'bn_{0}_{1}_branch_'.format(str(stage), block)\n",
    "    \n",
    "    x = tf.layers.conv2d(input_tensor, filter1, (1,1), strides=strides, name=conv_name + '2a') #applied strides\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2a')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filter2, kernel_size, padding='same', name=conv_name + '2b')\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2b')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filter3, (1,1), padding='same', name=conv_name + '2c')\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name=bn_name +'2c')\n",
    "    \n",
    "    shortcut = tf.layers.conv2d(input_tensor, filter3, (1,1), strides=strides, name=conv_name + '1')\n",
    "    shortcut = tf.layers.batch_normalization(shortcut, axis=bn_axis, name=bn_name +'1')\n",
    "    \n",
    "    x = tf.add(x, shortcut) #short cut connection\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inspired by resnet50 (infact, trying to recreate resnet50)\n",
    "def build_model(image_shape, n_classes):\n",
    "    \n",
    "    bn_axis = 3\n",
    "    #prepare input tensors\n",
    "    x = tf.placeholder(tf.float32, shape=[None, *image_shape], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    #prepare model\n",
    "    x = tf.layers.conv2d(x, 64, (7,7), strides=(2,2), name='conv1')\n",
    "    x = tf.layers.batch_normalization(x, axis=bn_axis, name='bn_conv1')\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x, (3,3), strides=(2,2))\n",
    "    \n",
    "    x = conv_block(x, 3, [64,64,256], stage=2, block='a', strides=(1,1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "    \n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "    \n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    \n",
    "    x = conv_block(x, 3, [512,512,2048], stage=2, block='a')\n",
    "    x = identity_block(x, 3, [512,512,2048], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [512,512,2048], stage=2, block='c')\n",
    "    \n",
    "    x = tf.layers.average_pooling2d(x, (7,7), name='avg_pool')\n",
    "    \n",
    "    #flattening\n",
    "    image_size = x.get_shape()[1:].num_elements()\n",
    "    x = tf.reshape(x, [-1, image_size])\n",
    "    \n",
    "    x = tf.layers.dense(x, n_classes, activation='softmax', name='fc5270')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
